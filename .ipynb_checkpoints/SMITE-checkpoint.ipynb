{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-anthropology",
   "metadata": {
    "id": "dated-anthropology"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 500\n",
    "\n",
    "%matplotlib inline\n",
    "df_clients = pd.read_csv('clients.csv', index_col='client_id')\n",
    "df_train = pd.read_csv('uplift_train.csv', index_col='client_id')\n",
    "df_test = pd.read_csv('uplift_test.csv', index_col='client_id')\n",
    "\n",
    "\n",
    "df_features = df_clients.copy()\n",
    "df_features['first_issue_time'] = \\\n",
    "    (pd.to_datetime(df_features['first_issue_date'])\n",
    "     - pd.to_datetime(df_features['first_issue_date']).min()) / pd.Timedelta('365d')\n",
    "\n",
    "df_features['first_redeem_time'] = \\\n",
    "    (pd.to_datetime(df_features['first_redeem_date'])\n",
    "     - pd.to_datetime(df_features['first_redeem_date']).min()) / pd.Timedelta('365d')\n",
    "\n",
    "df_features['issue_redeem_delay'] = df_features['first_redeem_time'] \\\n",
    "    - df_features['first_issue_time']\n",
    "\n",
    "df_features = df_features.join(pd.get_dummies(df_features['gender']))\n",
    "df_features['first_redeem_time'] = df_features['first_redeem_time'].fillna(df_features['first_redeem_time'].mean())\n",
    "df_features['issue_redeem_delay'] = df_features['issue_redeem_delay'].fillna(df_features['issue_redeem_delay'].mean())\n",
    "\n",
    "df_features = df_features.drop(['first_issue_date', 'first_redeem_date', 'gender'], axis=1)\n",
    "\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "\n",
    "X_train = df_features.loc[indices_learn, :]\n",
    "y_train = df_train.loc[indices_learn, 'target']\n",
    "treat_train = df_train.loc[indices_learn, 'treatment_flg']\n",
    "\n",
    "X_val = df_features.loc[indices_valid, :]\n",
    "y_val = df_train.loc[indices_valid, 'target']\n",
    "treat_val =  df_train.loc[indices_valid, 'treatment_flg']\n",
    "\n",
    "X_train_full = df_features.loc[indices_train, :]\n",
    "y_train_full = df_train.loc[:, 'target']\n",
    "treat_train_full = df_train.loc[:, 'treatment_flg']\n",
    "\n",
    "X_test = df_features.loc[indices_test, :]\n",
    "\n",
    "cat_features = ['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ob6NaE5VU-fK",
   "metadata": {
    "id": "Ob6NaE5VU-fK"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class X5Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, target=None, treatment=None):\n",
    "        super(X5Dataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data.values).type(torch.FloatTensor)\n",
    "        if target is not None:\n",
    "            self.target = torch.from_numpy(target.values).type(torch.FloatTensor)\n",
    "        if treatment is not None:\n",
    "            self.treatment = torch.from_numpy(treatment.values).type(torch.FloatTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        m0 = torch.zeros((1, self.data.shape[1] + 1))\n",
    "        m0[:, :-1] = self.data[idx, :]\n",
    "        m0[:, -1] = 0\n",
    "        m1 = torch.zeros((1, self.data.shape[1] + 1))\n",
    "        m1[:, :-1] = self.data[idx, :]\n",
    "        m1[:, -1] = 1\n",
    "\n",
    "        if self.target is None:\n",
    "            return m0.squeeze().to(device), m1.squeeze().to(device)\n",
    "        else:\n",
    "            return (m0.squeeze().to(device), m1.squeeze().to(device), \n",
    "                    self.target[idx].to(device), self.treatment[idx].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uRRkknrWYSRT",
   "metadata": {
    "id": "uRRkknrWYSRT"
   },
   "outputs": [],
   "source": [
    "train_dataset = X5Dataset(X_train, y_train, treat_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = X5Dataset(X_val, y_val, treat_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zTRmG4jZ9uHb",
   "metadata": {
    "id": "zTRmG4jZ9uHb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#class SMITEEncoder(nn.Model):\n",
    "\n",
    "#    def __init__(self, input_dim, hidden_dims):\n",
    "#        super(SMITEEncoder, self).__init__()\n",
    "#        self.net = \n",
    "\n",
    "#    def init_base_network(self, dims):\n",
    "#        layers = []\n",
    "#        for i in range(len(dims) - 1):\n",
    "#            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "#            layers.append(nn.ReLU())\n",
    "#        return nn.Sequential(*layers[:-1])\n",
    "\n",
    "p = 0.5\n",
    "\n",
    "\n",
    "def loss_function(y_true, preds0, preds1, alpha, T, p):\n",
    "    z = y_true * (T - p) / (p * (1 - p))\n",
    "    vec = (z - preds1 + preds0)\n",
    "    J = vec.matmul(vec) / len(vec)\n",
    "    preds = T * preds1 + (1 - T) * preds0\n",
    "    L = nn.functional.binary_cross_entropy(preds, y_true)\n",
    "    return alpha * L + (1 - alpha) * J\n",
    "\n",
    "input_dim = 8\n",
    "hidden_dims = [256, 256, 256, 256, 256, 256, 256]\n",
    "dims = [input_dim] + hidden_dims + [1]\n",
    "\n",
    "layers = []\n",
    "for i in range(len(dims) - 1):\n",
    "    layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "    layers.append(nn.ReLU())\n",
    "layers[-1] = nn.Sigmoid()\n",
    "model = nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TIs3-KJ5YxYx",
   "metadata": {
    "id": "TIs3-KJ5YxYx"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l4Uc8HX2S7CG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4Uc8HX2S7CG",
    "outputId": "f3106cfc-0ade-4afb-de54-3d469ed3e832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t Loss train 2.2962 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 1 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
      "Epoch 2 \t Loss train 2.2965 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 3 \t Loss train 2.2961 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 4 \t Loss train 2.2949 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 5 \t Loss train 2.2980 \t Accuracy test 0.379507 \t Loss test 2.2970\n",
      "Epoch 6 \t Loss train 2.2977 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
      "Epoch 7 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
      "Epoch 8 \t Loss train 2.2981 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
      "Epoch 9 \t Loss train 2.2981 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 10 \t Loss train 2.2968 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
      "Epoch 11 \t Loss train 2.2954 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 12 \t Loss train 2.2973 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
      "Epoch 13 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
      "Epoch 14 \t Loss train 2.2960 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
      "Epoch 15 \t Loss train 2.2959 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 16 \t Loss train 2.2939 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 17 \t Loss train 2.2949 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 18 \t Loss train 2.2958 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 19 \t Loss train 2.2948 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 20 \t Loss train 2.2974 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
      "Epoch 21 \t Loss train 2.2951 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 22 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
      "Epoch 23 \t Loss train 2.2971 \t Accuracy test 0.379507 \t Loss test 2.2965\n",
      "Epoch 24 \t Loss train 2.2971 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 25 \t Loss train 2.2962 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 26 \t Loss train 2.2960 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 27 \t Loss train 2.2954 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
      "Epoch 28 \t Loss train 2.2957 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
      "Epoch 29 \t Loss train 2.2957 \t Accuracy test 0.379507 \t Loss test 2.2962\n",
      "Epoch 30 \t Loss train 2.2975 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
      "Epoch 31 \t Loss train 2.2971 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 32 \t Loss train 2.2964 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 33 \t Loss train 2.2929 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 34 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 35 \t Loss train 2.2969 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 36 \t Loss train 2.2938 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 37 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 38 \t Loss train 2.2941 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 39 \t Loss train 2.2969 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
      "Epoch 40 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
      "Epoch 41 \t Loss train 2.2965 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 42 \t Loss train 2.2968 \t Accuracy test 0.379507 \t Loss test 2.2963\n",
      "Epoch 43 \t Loss train 2.2943 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 44 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
      "Epoch 45 \t Loss train 2.2961 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 46 \t Loss train 2.2956 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 47 \t Loss train 2.2961 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
      "Epoch 48 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
      "Epoch 49 \t Loss train 2.2938 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 50 \t Loss train 2.2943 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 51 \t Loss train 2.2968 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
      "Epoch 52 \t Loss train 2.2940 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
      "Epoch 53 \t Loss train 2.2969 \t Accuracy test 0.379507 \t Loss test 2.2959\n",
      "Epoch 54 \t Loss train 2.2934 \t Accuracy test 0.379507 \t Loss test 2.2961\n",
      "Epoch 55 \t Loss train 2.2958 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 56 \t Loss train 2.2948 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 57 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
      "Epoch 58 \t Loss train 2.2936 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
      "Epoch 59 \t Loss train 2.2945 \t Accuracy test 0.379507 \t Loss test 2.2963\n",
      "Epoch 60 \t Loss train 2.2974 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
      "Epoch 61 \t Loss train 2.2953 \t Accuracy test 0.379507 \t Loss test 2.2971\n",
      "Epoch 62 \t Loss train 2.2953 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
      "Epoch 63 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
      "Epoch 64 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 65 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2959\n",
      "Epoch 66 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2962\n",
      "Epoch 67 \t Loss train 2.2952 \t Accuracy test 0.379507 \t Loss test 2.2966\n",
      "Epoch 68 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2965\n",
      "Epoch 69 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
      "Epoch 70 \t Loss train 2.2934 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 71 \t Loss train 2.2924 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 72 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
      "Epoch 73 \t Loss train 2.2941 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 74 \t Loss train 2.2938 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 75 \t Loss train 2.2941 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
      "Epoch 76 \t Loss train 2.2960 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 77 \t Loss train 2.2932 \t Accuracy test 0.379507 \t Loss test 2.2961\n",
      "Epoch 78 \t Loss train 2.2924 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
      "Epoch 79 \t Loss train 2.2936 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
      "Epoch 80 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2965\n",
      "Epoch 81 \t Loss train 2.2935 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
      "Epoch 82 \t Loss train 2.2935 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 83 \t Loss train 2.2945 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
      "Epoch 84 \t Loss train 2.2932 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
      "Epoch 85 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
      "Epoch 86 \t Loss train 2.2925 \t Accuracy test 0.379507 \t Loss test 2.2961\n",
      "Epoch 87 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 88 \t Loss train 2.2930 \t Accuracy test 0.379507 \t Loss test 2.2970\n",
      "Epoch 89 \t Loss train 2.2920 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
      "Epoch 90 \t Loss train 2.2935 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 91 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
      "Epoch 92 \t Loss train 2.2915 \t Accuracy test 0.379507 \t Loss test 2.2975\n",
      "Epoch 93 \t Loss train 2.2937 \t Accuracy test 0.379507 \t Loss test 2.2963\n",
      "Epoch 94 \t Loss train 2.2943 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
      "Epoch 95 \t Loss train 2.2932 \t Accuracy test 0.379507 \t Loss test 2.2973\n",
      "Epoch 96 \t Loss train 2.2936 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
      "Epoch 97 \t Loss train 2.2946 \t Accuracy test 0.379507 \t Loss test 2.2974\n",
      "Epoch 98 \t Loss train 2.2925 \t Accuracy test 0.379507 \t Loss test 2.2983\n",
      "Epoch 99 \t Loss train 2.2962 \t Accuracy test 0.379507 \t Loss test 2.2985\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "alpha = 0.1\n",
    "p = 0.5\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "\n",
    "    loss_train = []\n",
    "\n",
    "    for data0, data1, target, treat in train_loader:\n",
    "        \n",
    "        preds0 = model(data0).squeeze()\n",
    "        preds1 = model(data1).squeeze()\n",
    "        loss = loss_function(target, preds0, preds1, alpha, treat, p)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_train.append(loss.item())\n",
    "    loss_train = np.mean(loss_train)\n",
    "\n",
    "    labels_val = []\n",
    "    preds_val = []\n",
    "    loss_val = []\n",
    "    for data0, data1, target, treat in val_loader:\n",
    "        labels_val += list(target)\n",
    "        preds0 = model(data0).squeeze()\n",
    "        preds1 = model(data1).squeeze()\n",
    "        loss = loss_function(target, preds0, preds1, alpha, treat, p)\n",
    "        preds_val += list(preds1 - preds0)\n",
    "        loss_val.append(loss.item())\n",
    "\n",
    "    accuracy = (np.array(preds_val).astype(np.int16) == np.array(labels_val)).sum() / len(preds_val)\n",
    "    print(f'Epoch {i} \\t Loss train {loss_train:.4f} \\t Accuracy test {accuracy:4f} \\t Loss test {np.mean(loss_val):.4f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SMITE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
