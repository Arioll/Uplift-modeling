{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "better-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklift.metrics import uplift_at_k\n",
    "from sklift.viz import plot_uplift_curve\n",
    "from sklift.viz import plot_qini_curve\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from causalml.inference.meta import BaseXRegressor, BaseTClassifier, BaseSClassifier, BaseRClassifier\n",
    "from causalml.dataset import *\n",
    "from causalml.metrics import *\n",
    "\n",
    "from sklift.models import TwoModels\n",
    "from sklift.models import SoloModel\n",
    "\n",
    "from econml.metalearners import TLearner\n",
    "from econml.metalearners import SLearner\n",
    "from econml.metalearners import XLearner\n",
    "\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-municipality",
   "metadata": {},
   "source": [
    "# Meta-Learners by Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "measured-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CausalML(X_train, treat_train, y_train, X_test, treat_test, y_test):\n",
    "    learner_t = BaseTClassifier(learner=LGBMClassifier())\n",
    "    learner_t.fit(X=X_train, treatment=treat_train, y=y_train)\n",
    "    cate_t = np.squeeze(learner_t.predict(X_test))\n",
    "    score_t = uplift_at_k(y_true=y_test, uplift=cate_t, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "    \n",
    "    learner_s = BaseSClassifier(learner=LGBMClassifier())\n",
    "    learner_s.fit(X=X_train, treatment=treat_train, y=y_train)\n",
    "    cate_s = np.squeeze(learner_s.predict(X_test))\n",
    "    score_s = uplift_at_k(y_true=y_test, uplift=cate_s, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "    \n",
    "    learner_x = BaseXRegressor(LGBMClassifier(),LGBMClassifier())\n",
    "    learner_x.fit(X=X_train, treatment=treat_train, y=y_train)\n",
    "    cate_x = np.squeeze(learner_x.predict(X_test))\n",
    "    score_x = uplift_at_k(y_true=y_test, uplift=cate_x, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "    \n",
    "    return score_t, score_s, score_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sonic-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SkLift(X_train, treat_train, y_train, X_test, treat_test, y_test):\n",
    "    tm = TwoModels(estimator_trmnt=LGBMClassifier(), estimator_ctrl=LGBMClassifier(), method='vanilla')\n",
    "    tm = tm.fit(X_train, y_train, treat_train)\n",
    "    uplift_tm = tm.predict(X_test)\n",
    "    score_t = uplift_at_k(y_true=y_test, uplift=uplift_tm, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "\n",
    "    sm = SoloModel(LGBMClassifier())\n",
    "    sm = sm.fit(X_train, y_train, treat_train)\n",
    "    uplift_sm = sm.predict(X_test)\n",
    "    score_s = uplift_at_k(y_true=y_test, uplift=uplift_sm, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "    \n",
    "    score_x = 0\n",
    "    \n",
    "    return score_t, score_s, score_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thousand-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EconML(X_train, treat_train, y_train, X_test, treat_test, y_test):\n",
    "    est = TLearner(LGBMClassifier())\n",
    "    est.fit(y_train, treat_train, X_train)\n",
    "    uplift = np.squeeze(est.const_marginal_effect(X_test))\n",
    "    score_t = uplift_at_k(y_true=y_test, uplift=uplift, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "\n",
    "    est = SLearner(LGBMClassifier())\n",
    "    est.fit(y_train, treat_train, X_train)\n",
    "    uplift = np.squeeze(est.const_marginal_effect(X_test))\n",
    "    score_s = uplift_at_k(y_true=y_test, uplift=uplift, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "    \n",
    "    est = XLearner(LGBMClassifier())\n",
    "    est.fit(y_train, treat_train, X_train)\n",
    "    uplift = np.squeeze(est.const_marginal_effect(X_test))\n",
    "    score_x = uplift_at_k(y_true=y_test, uplift=uplift, treatment=treat_test, strategy='by_group', k=0.3)\n",
    "    \n",
    "    return score_t, score_s, score_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "irish-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "Libraries = [CausalML, SkLift, EconML]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-click",
   "metadata": {},
   "source": [
    "# Preprocessing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "minimal-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-suicide",
   "metadata": {},
   "source": [
    "## X5 Retail Hero Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nominated-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients = pd.read_csv('clients.csv', index_col='client_id')\n",
    "df_train = pd.read_csv('uplift_train.csv', index_col='client_id')\n",
    "df_test = pd.read_csv('uplift_test.csv', index_col='client_id')\n",
    "\n",
    "df_features = df_clients.copy()\n",
    "df_features['first_issue_time'] = \\\n",
    "    (pd.to_datetime(df_features['first_issue_date'])\n",
    "     - pd.to_datetime(df_features['first_issue_date']).min()) / pd.Timedelta('365d')\n",
    "\n",
    "df_features['first_redeem_time'] = \\\n",
    "    (pd.to_datetime(df_features['first_redeem_date'])\n",
    "     - pd.to_datetime(df_features['first_redeem_date']).min()) / pd.Timedelta('365d')\n",
    "\n",
    "df_features['issue_redeem_delay'] = df_features['first_redeem_time'] \\\n",
    "    - df_features['first_issue_time']\n",
    "\n",
    "df_features = df_features.join(pd.get_dummies(df_features['gender']))\n",
    "df_features['first_redeem_time'] = df_features['first_redeem_time'].fillna(df_features['first_redeem_time'].mean())\n",
    "df_features['issue_redeem_delay'] = df_features['issue_redeem_delay'].fillna(df_features['issue_redeem_delay'].mean())\n",
    "\n",
    "df_features = df_features.drop(['first_issue_date', 'first_redeem_date', 'gender'], axis=1)\n",
    "\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_testid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "X_train = df_features.loc[indices_learn, :]\n",
    "y_train = df_train.loc[indices_learn, 'target']\n",
    "treat_train = df_train.loc[indices_learn, 'treatment_flg']\n",
    "\n",
    "X_test = df_features.loc[indices_testid, :]\n",
    "y_test = df_train.loc[indices_testid, 'target']\n",
    "treat_test =  df_train.loc[indices_testid, 'treatment_flg']\n",
    "\n",
    "X_train_full = df_features.loc[indices_train, :]\n",
    "y_train_full = df_train.loc[:, 'target']\n",
    "treat_train_full = df_train.loc[:, 'treatment_flg']\n",
    "\n",
    "cat_features = ['gender']\n",
    "\n",
    "Datasets.append((X_train, treat_train, y_train, X_test, treat_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-shade",
   "metadata": {},
   "source": [
    "## Hillstrom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "simplified-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hillstrom.csv')\n",
    "df.drop(['history_segment', \"conversion\", \"spend\"], axis=1, inplace=True)\n",
    "\n",
    "cat_cols = ['zip_code', 'channel']\n",
    "df_ohe = pd.get_dummies(df, columns=cat_cols)\n",
    "df_ohe.segment = df_ohe.segment.map({'Womens E-Mail': 1, 'Mens E-Mail': 1, 'No E-Mail': 0})\n",
    "\n",
    "X = df_ohe.drop('visit', axis=1)\n",
    "y = df_ohe['visit'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "treat_train = X_train['segment']\n",
    "treat_test = X_test['segment']\n",
    "\n",
    "X_train.drop(['segment'], axis=1, inplace=True)\n",
    "X_test.drop(['segment'], axis=1, inplace=True)\n",
    "\n",
    "Datasets.append((X_train, treat_train, y_train, X_test, treat_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-gardening",
   "metadata": {},
   "source": [
    "## Kuusito Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alpine-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Kuusito.csv')\n",
    "df.drop(['customer_type'], axis=1, inplace=True)\n",
    "\n",
    "df = df.replace(r'Value', '', regex=True)\n",
    "df['target_control'] = df['target_control'].map({'control': 1, 'target': 0})\n",
    "df['outcome'] = df['outcome'].map({'negative': 0, 'positive': 1})\n",
    "\n",
    "df = pd.get_dummies(df,drop_first=True)\n",
    "\n",
    "X = df.drop('outcome', axis=1).astype('int64')\n",
    "y = df['outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "treat_train = X_train['target_control']\n",
    "treat_test = X_test['target_control']\n",
    "\n",
    "X_train.drop(['target_control'], axis=1, inplace=True)\n",
    "X_test.drop(['target_control'], axis=1, inplace=True)\n",
    "X_train.drop(['customer_id'], axis=1, inplace=True)\n",
    "X_test.drop(['customer_id'], axis=1, inplace=True)\n",
    "\n",
    "Datasets.append((X_train, treat_train, y_train, X_test, treat_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-tract",
   "metadata": {},
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "naval-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X, treatment, tau, b, e = synthetic_data(mode=2, n=10000, p=8, sigma=1.0)\n",
    "y = (y > np.median(y)).astype(int)\n",
    "X_train, X_test, y_train, y_test, treat_train, treat_test= train_test_split(X, y, treatment, test_size=0.33, random_state=0)\n",
    "\n",
    "Datasets.append((X_train, treat_train, y_train, X_test, treat_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-timing",
   "metadata": {},
   "source": [
    "# Filling in the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sunrise-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.MultiIndex.from_product([['RetailHero', 'Hillstrom', 'Kuusito', 'Synthetic'],\n",
    "                                  ['T', 'S', 'X']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "veterinary-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros((12, 3))\n",
    "\n",
    "for di, (X_train, treat_train, y_train, X_test, treat_test, y_test) in enumerate(Datasets):\n",
    "    for fi, MetaLearners in enumerate(Libraries):\n",
    "        scores[3*di:3*di+3, fi] = MetaLearners(X_train, treat_train, y_train, X_test, treat_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confused-fence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Library</th>\n",
       "      <th>CausalML</th>\n",
       "      <th>SkLift</th>\n",
       "      <th>EconML</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th>Learner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">RetailHero</th>\n",
       "      <th>T</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.038</td>\n",
       "      <td>-</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Hillstrom</th>\n",
       "      <th>T</th>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.067</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.073</td>\n",
       "      <td>-</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Kuusito</th>\n",
       "      <th>T</th>\n",
       "      <td>0.279</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.310</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.239</td>\n",
       "      <td>-</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Synthetic</th>\n",
       "      <th>T</th>\n",
       "      <td>0.420</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.458</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.361</td>\n",
       "      <td>-</td>\n",
       "      <td>0.361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Library             CausalML SkLift  EconML\n",
       "Dataset    Learner                         \n",
       "RetailHero T           0.053  0.053   0.038\n",
       "           S           0.040   0.04   0.038\n",
       "           X           0.038      -   0.038\n",
       "Hillstrom  T           0.061  0.061   0.074\n",
       "           S           0.067  0.067   0.074\n",
       "           X           0.073      -   0.073\n",
       "Kuusito    T           0.279  0.279   0.219\n",
       "           S           0.310   0.31   0.183\n",
       "           X           0.239      -   0.239\n",
       "Synthetic  T           0.420   0.42   0.377\n",
       "           S           0.458  0.458   0.345\n",
       "           X           0.361      -   0.361"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(scores,\n",
    "                  columns=['CausalML', 'SkLift', 'EconML'],\n",
    "                  index=idx)\n",
    "\n",
    "df.index.names = ['Dataset', 'Learner']\n",
    "df.columns.name = 'Library'\n",
    "df = df.round(3)\n",
    "df.replace(0, '-', inplace=True)\n",
    "display(df)\n",
    "\n",
    "with open(\"LearnersByLibraries.txt\", \"w\") as text_file:\n",
    "    text_file.write(df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
