{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "SMITE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dated-anthropology"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 500\n",
        "\n",
        "%matplotlib inline\n",
        "df_clients = pd.read_csv('clients.csv', index_col='client_id')\n",
        "df_train = pd.read_csv('uplift_train.csv', index_col='client_id')\n",
        "df_test = pd.read_csv('uplift_test.csv', index_col='client_id')\n",
        "\n",
        "\n",
        "df_features = df_clients.copy()\n",
        "df_features['first_issue_time'] = \\\n",
        "    (pd.to_datetime(df_features['first_issue_date'])\n",
        "     - pd.to_datetime(df_features['first_issue_date']).min()) / pd.Timedelta('365d')\n",
        "\n",
        "df_features['first_redeem_time'] = \\\n",
        "    (pd.to_datetime(df_features['first_redeem_date'])\n",
        "     - pd.to_datetime(df_features['first_redeem_date']).min()) / pd.Timedelta('365d')\n",
        "\n",
        "df_features['issue_redeem_delay'] = df_features['first_redeem_time'] \\\n",
        "    - df_features['first_issue_time']\n",
        "\n",
        "df_features = df_features.join(pd.get_dummies(df_features['gender']))\n",
        "df_features['first_redeem_time'] = df_features['first_redeem_time'].fillna(df_features['first_redeem_time'].mean())\n",
        "df_features['issue_redeem_delay'] = df_features['issue_redeem_delay'].fillna(df_features['issue_redeem_delay'].mean())\n",
        "\n",
        "df_features = df_features.drop(['first_issue_date', 'first_redeem_date', 'gender'], axis=1)\n",
        "\n",
        "indices_train = df_train.index\n",
        "indices_test = df_test.index\n",
        "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
        "\n",
        "\n",
        "X_train = df_features.loc[indices_learn, :]\n",
        "y_train = df_train.loc[indices_learn, 'target']\n",
        "treat_train = df_train.loc[indices_learn, 'treatment_flg']\n",
        "\n",
        "X_val = df_features.loc[indices_valid, :]\n",
        "y_val = df_train.loc[indices_valid, 'target']\n",
        "treat_val =  df_train.loc[indices_valid, 'treatment_flg']\n",
        "\n",
        "X_train_full = df_features.loc[indices_train, :]\n",
        "y_train_full = df_train.loc[:, 'target']\n",
        "treat_train_full = df_train.loc[:, 'treatment_flg']\n",
        "\n",
        "X_test = df_features.loc[indices_test, :]\n",
        "\n",
        "cat_features = ['gender']"
      ],
      "id": "dated-anthropology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob6NaE5VU-fK"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class X5Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, target=None, treatment=None):\n",
        "        super(X5Dataset, self).__init__()\n",
        "        self.data = torch.from_numpy(data.values).type(torch.FloatTensor)\n",
        "        if target is not None:\n",
        "            self.target = torch.from_numpy(target.values).type(torch.FloatTensor)\n",
        "        if treatment is not None:\n",
        "            self.treatment = torch.from_numpy(treatment.values).type(torch.FloatTensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        m0 = torch.zeros((1, self.data.shape[1] + 1))\n",
        "        m0[:, :-1] = self.data[idx, :]\n",
        "        m0[:, -1] = 0\n",
        "        m1 = torch.zeros((1, self.data.shape[1] + 1))\n",
        "        m1[:, :-1] = self.data[idx, :]\n",
        "        m1[:, -1] = 1\n",
        "\n",
        "        if self.target is None:\n",
        "            return m0.squeeze().to(device), m1.squeeze().to(device)\n",
        "        else:\n",
        "            return (m0.squeeze().to(device), m1.squeeze().to(device), \n",
        "                    self.target[idx].to(device), self.treatment[idx].to(device))"
      ],
      "id": "Ob6NaE5VU-fK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRRkknrWYSRT"
      },
      "source": [
        "train_dataset = X5Dataset(X_train, y_train, treat_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = X5Dataset(X_val, y_val, treat_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "id": "uRRkknrWYSRT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTRmG4jZ9uHb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#class SMITEEncoder(nn.Model):\n",
        "\n",
        "#    def __init__(self, input_dim, hidden_dims):\n",
        "#        super(SMITEEncoder, self).__init__()\n",
        "#        self.net = \n",
        "\n",
        "#    def init_base_network(self, dims):\n",
        "#        layers = []\n",
        "#        for i in range(len(dims) - 1):\n",
        "#            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "#            layers.append(nn.ReLU())\n",
        "#        return nn.Sequential(*layers[:-1])\n",
        "\n",
        "p = 0.5\n",
        "\n",
        "\n",
        "def loss_function(y_true, preds0, preds1, alpha, T, p):\n",
        "    z = y_true * (T - p) / (p * (1 - p))\n",
        "    vec = (z - preds1 + preds0)\n",
        "    J = vec.matmul(vec) / len(vec)\n",
        "    preds = T * preds1 + (1 - T) * preds0\n",
        "    L = nn.functional.binary_cross_entropy(preds, y_true)\n",
        "    return alpha * L + (1 - alpha) * J\n",
        "\n",
        "input_dim = 8\n",
        "hidden_dims = [256, 256, 256, 256, 256, 256, 256]\n",
        "dims = [input_dim] + hidden_dims + [1]\n",
        "\n",
        "layers = []\n",
        "for i in range(len(dims) - 1):\n",
        "    layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "    layers.append(nn.ReLU())\n",
        "layers[-1] = nn.Sigmoid()\n",
        "model = nn.Sequential(*layers).to(device)"
      ],
      "id": "zTRmG4jZ9uHb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIs3-KJ5YxYx"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), 0.001)"
      ],
      "id": "TIs3-KJ5YxYx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Uc8HX2S7CG",
        "outputId": "f3106cfc-0ade-4afb-de54-3d469ed3e832"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "alpha = 0.1\n",
        "p = 0.5\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "\n",
        "    loss_train = []\n",
        "\n",
        "    for data0, data1, target, treat in train_loader:\n",
        "        \n",
        "        preds0 = model(data0).squeeze()\n",
        "        preds1 = model(data1).squeeze()\n",
        "        loss = loss_function(target, preds0, preds1, alpha, treat, p)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss_train.append(loss.item())\n",
        "    loss_train = np.mean(loss_train)\n",
        "\n",
        "    labels_val = []\n",
        "    preds_val = []\n",
        "    loss_val = []\n",
        "    for data0, data1, target, treat in val_loader:\n",
        "        labels_val += list(target)\n",
        "        preds0 = model(data0).squeeze()\n",
        "        preds1 = model(data1).squeeze()\n",
        "        loss = loss_function(target, preds0, preds1, alpha, treat, p)\n",
        "        preds_val += list(preds1 - preds0)\n",
        "        loss_val.append(loss.item())\n",
        "\n",
        "    accuracy = (np.array(preds_val).astype(np.int16) == np.array(labels_val)).sum() / len(preds_val)\n",
        "    print(f'Epoch {i} \\t Loss train {loss_train:.4f} \\t Accuracy test {accuracy:4f} \\t Loss test {np.mean(loss_val):.4f}')"
      ],
      "id": "l4Uc8HX2S7CG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Loss train 2.2962 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 1 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
            "Epoch 2 \t Loss train 2.2965 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 3 \t Loss train 2.2961 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 4 \t Loss train 2.2949 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 5 \t Loss train 2.2980 \t Accuracy test 0.379507 \t Loss test 2.2970\n",
            "Epoch 6 \t Loss train 2.2977 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
            "Epoch 7 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
            "Epoch 8 \t Loss train 2.2981 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
            "Epoch 9 \t Loss train 2.2981 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 10 \t Loss train 2.2968 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
            "Epoch 11 \t Loss train 2.2954 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 12 \t Loss train 2.2973 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
            "Epoch 13 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
            "Epoch 14 \t Loss train 2.2960 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
            "Epoch 15 \t Loss train 2.2959 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 16 \t Loss train 2.2939 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 17 \t Loss train 2.2949 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 18 \t Loss train 2.2958 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 19 \t Loss train 2.2948 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 20 \t Loss train 2.2974 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
            "Epoch 21 \t Loss train 2.2951 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 22 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2967\n",
            "Epoch 23 \t Loss train 2.2971 \t Accuracy test 0.379507 \t Loss test 2.2965\n",
            "Epoch 24 \t Loss train 2.2971 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 25 \t Loss train 2.2962 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 26 \t Loss train 2.2960 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 27 \t Loss train 2.2954 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
            "Epoch 28 \t Loss train 2.2957 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
            "Epoch 29 \t Loss train 2.2957 \t Accuracy test 0.379507 \t Loss test 2.2962\n",
            "Epoch 30 \t Loss train 2.2975 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
            "Epoch 31 \t Loss train 2.2971 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 32 \t Loss train 2.2964 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 33 \t Loss train 2.2929 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 34 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 35 \t Loss train 2.2969 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 36 \t Loss train 2.2938 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 37 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 38 \t Loss train 2.2941 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 39 \t Loss train 2.2969 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
            "Epoch 40 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
            "Epoch 41 \t Loss train 2.2965 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 42 \t Loss train 2.2968 \t Accuracy test 0.379507 \t Loss test 2.2963\n",
            "Epoch 43 \t Loss train 2.2943 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 44 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
            "Epoch 45 \t Loss train 2.2961 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 46 \t Loss train 2.2956 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 47 \t Loss train 2.2961 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
            "Epoch 48 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
            "Epoch 49 \t Loss train 2.2938 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 50 \t Loss train 2.2943 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 51 \t Loss train 2.2968 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
            "Epoch 52 \t Loss train 2.2940 \t Accuracy test 0.379507 \t Loss test 2.2954\n",
            "Epoch 53 \t Loss train 2.2969 \t Accuracy test 0.379507 \t Loss test 2.2959\n",
            "Epoch 54 \t Loss train 2.2934 \t Accuracy test 0.379507 \t Loss test 2.2961\n",
            "Epoch 55 \t Loss train 2.2958 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 56 \t Loss train 2.2948 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 57 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
            "Epoch 58 \t Loss train 2.2936 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
            "Epoch 59 \t Loss train 2.2945 \t Accuracy test 0.379507 \t Loss test 2.2963\n",
            "Epoch 60 \t Loss train 2.2974 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
            "Epoch 61 \t Loss train 2.2953 \t Accuracy test 0.379507 \t Loss test 2.2971\n",
            "Epoch 62 \t Loss train 2.2953 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
            "Epoch 63 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
            "Epoch 64 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 65 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2959\n",
            "Epoch 66 \t Loss train 2.2955 \t Accuracy test 0.379507 \t Loss test 2.2962\n",
            "Epoch 67 \t Loss train 2.2952 \t Accuracy test 0.379507 \t Loss test 2.2966\n",
            "Epoch 68 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2965\n",
            "Epoch 69 \t Loss train 2.2963 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
            "Epoch 70 \t Loss train 2.2934 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 71 \t Loss train 2.2924 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 72 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
            "Epoch 73 \t Loss train 2.2941 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 74 \t Loss train 2.2938 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 75 \t Loss train 2.2941 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
            "Epoch 76 \t Loss train 2.2960 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 77 \t Loss train 2.2932 \t Accuracy test 0.379507 \t Loss test 2.2961\n",
            "Epoch 78 \t Loss train 2.2924 \t Accuracy test 0.379507 \t Loss test 2.2958\n",
            "Epoch 79 \t Loss train 2.2936 \t Accuracy test 0.379507 \t Loss test 2.2960\n",
            "Epoch 80 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2965\n",
            "Epoch 81 \t Loss train 2.2935 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
            "Epoch 82 \t Loss train 2.2935 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 83 \t Loss train 2.2945 \t Accuracy test 0.379507 \t Loss test 2.2956\n",
            "Epoch 84 \t Loss train 2.2932 \t Accuracy test 0.379507 \t Loss test 2.2953\n",
            "Epoch 85 \t Loss train 2.2947 \t Accuracy test 0.379507 \t Loss test 2.2952\n",
            "Epoch 86 \t Loss train 2.2925 \t Accuracy test 0.379507 \t Loss test 2.2961\n",
            "Epoch 87 \t Loss train 2.2950 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 88 \t Loss train 2.2930 \t Accuracy test 0.379507 \t Loss test 2.2970\n",
            "Epoch 89 \t Loss train 2.2920 \t Accuracy test 0.379507 \t Loss test 2.2957\n",
            "Epoch 90 \t Loss train 2.2935 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 91 \t Loss train 2.2944 \t Accuracy test 0.379507 \t Loss test 2.2955\n",
            "Epoch 92 \t Loss train 2.2915 \t Accuracy test 0.379507 \t Loss test 2.2975\n",
            "Epoch 93 \t Loss train 2.2937 \t Accuracy test 0.379507 \t Loss test 2.2963\n",
            "Epoch 94 \t Loss train 2.2943 \t Accuracy test 0.379507 \t Loss test 2.2968\n",
            "Epoch 95 \t Loss train 2.2932 \t Accuracy test 0.379507 \t Loss test 2.2973\n",
            "Epoch 96 \t Loss train 2.2936 \t Accuracy test 0.379507 \t Loss test 2.2964\n",
            "Epoch 97 \t Loss train 2.2946 \t Accuracy test 0.379507 \t Loss test 2.2974\n",
            "Epoch 98 \t Loss train 2.2925 \t Accuracy test 0.379507 \t Loss test 2.2983\n",
            "Epoch 99 \t Loss train 2.2962 \t Accuracy test 0.379507 \t Loss test 2.2985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thousand-hours",
        "outputId": "2c17c927-6b3b-41e6-8e78-51fb1c387258"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import to_categorical"
      ],
      "id": "thousand-hours",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "powered-disco"
      },
      "source": [
        "def build_base_network(input_shape):\n",
        "    \n",
        "    seq = Sequential()\n",
        "\n",
        "    #flatten \n",
        "    seq.add(Dense(128, activation='relu'))\n",
        "    seq.add(Dense(128, activation='relu'))\n",
        "    seq.add(Dense(50, activation='relu'))\n",
        "    return seq"
      ],
      "id": "powered-disco",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "physical-bottle"
      },
      "source": [
        "input_dim = (8, )\n",
        "input_a = Input(shape=input_dim)\n",
        "input_b = Input(shape=input_dim)\n",
        "\n",
        "base_network = build_base_network(input_dim)\n",
        "feat_vecs_a = base_network(input_a)\n",
        "feat_vecs_b = base_network(input_b)"
      ],
      "id": "physical-bottle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "behind-chile"
      },
      "source": [
        "# def output_uplift(vects):\n",
        "#     m0, m1 = vects\n",
        "    \n",
        "#     return m0, m1\n",
        "\n",
        "\n",
        "# def uplift_output_shape(shapes):\n",
        "#     shape1, shape2 = shapes\n",
        "#     return (shape1[0], 2)\n",
        "\n",
        "# uplift = Lambda(output_uplift, output_shape=uplift_output_shape)([feat_vecs_a, feat_vecs_b])"
      ],
      "id": "behind-chile",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intense-romania"
      },
      "source": [
        "epochs = 2\n",
        "rms = RMSprop()\n",
        "output = Dense(1, activation='sigmoid')(feat_vecs_a)\n",
        "\n",
        "model = Model(input=[input_a, input_b], output=[output, output])"
      ],
      "id": "intense-romania",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "interested-fraud",
        "outputId": "7376ae07-0ece-43c4-a08a-f7d76d08c332"
      },
      "source": [
        "def smite_loss(y_true, y_pred):\n",
        "    m0, m1 = y_pred[0], y_pred[1]\n",
        "    t = treat_train\n",
        "    mt = K.dot(t, m1) + K.dot((1 - t), m0)\n",
        "    z = [FlaggedFloat(t * o / 0.5 - (1 - t) * o / 0.5) for o, t in zip(y_true, t)]\n",
        "    J = K.mean(K.square(z - (m1 - m0)))\n",
        "    L = K.sum(y_true * K.log(mt) + (1 - y_true) * K.log(1 - mt))\n",
        "    return (1 - 0.2) * J + 0.2 * L \n",
        "\n",
        "model.compile(loss=smite_loss, optimizer=rms)"
      ],
      "id": "interested-fraud",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"loss_3/dense_4_loss/smite_loss/strided_slice_1:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'rank'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-09b3e6c875af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mJ\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmite_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m#                   layer losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# Functions for train, test and predict will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                     output_loss = loss_fn(\n\u001b[1;32m--> 692\u001b[1;33m                         y_true, y_pred, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lambda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<lambda>'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             return losses_utils.compute_weighted_loss(\n\u001b[0;32m     73\u001b[0m                 losses, sample_weight, reduction=self.reduction)\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-09b3e6c875af>\u001b[0m in \u001b[0;36msmite_loss\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mm0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtreat_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mFlaggedFloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mm0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1347\u001b[0m     \u001b[1;33m{\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mnp_implementation\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \"\"\"\n\u001b[1;32m-> 1349\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[0mx_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mndim\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;33m{\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mnp_implementation\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     \"\"\"\n\u001b[1;32m--> 848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "successful-constitutional"
      },
      "source": [
        "sample_1 = X_train.assign(treatment = 1)\n",
        "sample_2 = X_train.assign(treatment = 0) \n",
        "\n",
        "model.fit([sample_1, sample_2], y_train, validation_split=.25, batch_size=1, verbose=2, nb_epoch=epochs)"
      ],
      "id": "successful-constitutional",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suited-socket"
      },
      "source": [
        "model.predict([X_test.assign(treatment = 1), X_test.assign(treatment = 0)])"
      ],
      "id": "suited-socket",
      "execution_count": null,
      "outputs": []
    }
  ]
}