{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "SMITE_new_loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd4GqAxhLV_5",
        "outputId": "8d44af2c-bf55-4f95-fe68-e3bab4545059"
      },
      "source": [
        "!pip3 install scikit-uplift\n",
        "!pip3 install causalml"
      ],
      "id": "hd4GqAxhLV_5",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-uplift in /home/anton/.local/lib/python3.8/site-packages (0.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /home/anton/.local/lib/python3.8/site-packages (from scikit-uplift) (0.23.2)\n",
            "Requirement already satisfied: pandas in /home/anton/.local/lib/python3.8/site-packages (from scikit-uplift) (1.1.1)\n",
            "Requirement already satisfied: matplotlib in /home/anton/.local/lib/python3.8/site-packages (from scikit-uplift) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from scikit-uplift) (2.22.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /home/anton/.local/lib/python3.8/site-packages (from scikit-uplift) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/anton/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (0.12.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /home/anton/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anton/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (2.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas->scikit-uplift) (2019.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas->scikit-uplift) (2.7.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib->scikit-uplift) (7.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/anton/.local/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/anton/.local/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (0.10.0)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /home/anton/.local/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/lib/python3/dist-packages (from matplotlib->scikit-uplift) (2.4.6)\n",
            "Requirement already satisfied: six in /home/anton/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->scikit-uplift) (1.15.0)\n",
            "Processing /home/anton/.cache/pip/wheels/d7/16/cd/f9918ec891368bfcbbd4f6de9a612dfbe2b9d9c64e24c729a8/causalml-0.10.0-cp38-cp38-linux_x86_64.whl\n",
            "Requirement already satisfied: tqdm in /home/anton/.local/lib/python3.8/site-packages (from causalml) (4.49.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=0.16.0 in /home/anton/.local/lib/python3.8/site-packages (from causalml) (1.18.5)\n",
            "Requirement already satisfied: keras in /home/anton/.local/lib/python3.8/site-packages (from causalml) (2.4.3)\n",
            "Requirement already satisfied: lightgbm in /home/anton/.local/lib/python3.8/site-packages (from causalml) (3.2.0)\n",
            "Requirement already satisfied: dill in /home/anton/.local/lib/python3.8/site-packages (from causalml) (0.3.3)\n",
            "Requirement already satisfied: pyro-ppl in /home/anton/.local/lib/python3.8/site-packages (from causalml) (1.6.0)\n",
            "Collecting tensorflow>=1.15.2\n",
            "  Using cached tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_TdkxbR8INi"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "from sklift.metrics import uplift_at_k, qini_auc_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data_dir = 'data/'"
      ],
      "id": "6_TdkxbR8INi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTRmG4jZ9uHb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def indirect_loss_function(y_true, preds0, preds1, alpha, T, p, c=1e-7):\n",
        "\n",
        "    # preds0 is prediction for data with 0 treatment and preds1 is predictions for data with 1 treatment\n",
        "    denom1 = torch.maximum(preds0 + preds1, torch.ones(preds1.shape).to(device) * c)\n",
        "    Pi1 = preds1 / denom1\n",
        "    denom2 = torch.maximum((1 - preds0) + (1 - preds1), torch.ones(preds1.shape).to(device) * c)\n",
        "    Pi0 = (1 - preds1) / denom2\n",
        "    Pi = y_true * Pi1 + (1 - y_true) * Pi0\n",
        "    I = nn.functional.binary_cross_entropy(Pi, T)\n",
        "\n",
        "    preds = T * preds1 + (1 - T) * preds0\n",
        "    L = nn.functional.binary_cross_entropy(preds, y_true)\n",
        "\n",
        "    return alpha * L + (1 - alpha) * I\n",
        "\n",
        "def direct_loss_function(y_true, preds0, preds1, alpha, T, p):\n",
        "    #z = y_true * (T - p) / (p * (1 - p))\n",
        "    e_X = (T == 1).sum() / len(T) \n",
        "    z = (T * y_true) / e_X + ((1 - T) * y_true) / (1 - e_X)\n",
        "    vec = (z - preds1 + preds0)\n",
        "    J = vec.matmul(vec) / len(vec)\n",
        "    preds = T * preds1 + (1 - T) * preds0\n",
        "    L = nn.functional.binary_cross_entropy(preds, y_true)\n",
        "    return alpha * L + (1 - alpha) * J"
      ],
      "id": "zTRmG4jZ9uHb",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob6NaE5VU-fK"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class RealDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, target=None, treatment=None):\n",
        "        super(RealDataset, self).__init__()\n",
        "        self.data = torch.from_numpy(data.values).type(torch.FloatTensor)\n",
        "        if target is not None:\n",
        "            self.target = torch.from_numpy(target.values).type(torch.FloatTensor)\n",
        "        if treatment is not None:\n",
        "            self.treatment = torch.from_numpy(treatment.values).type(torch.FloatTensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Make loader to give row with 0 treatment and row with 1 treatment for one query\n",
        "\n",
        "        m0 = torch.zeros((1, self.data.shape[1] + 1))\n",
        "        m0[:, :-1] = self.data[idx, :]\n",
        "        m0[:, -1] = 0\n",
        "        m1 = torch.zeros((1, self.data.shape[1] + 1))\n",
        "        m1[:, :-1] = self.data[idx, :]\n",
        "        m1[:, -1] = 1\n",
        "\n",
        "        if self.target is None:\n",
        "            return m0.squeeze().to(device), m1.squeeze().to(device)\n",
        "        else:\n",
        "            return (m0.squeeze().to(device), m1.squeeze().to(device), \n",
        "                    self.target[idx].to(device), self.treatment[idx].to(device))"
      ],
      "id": "Ob6NaE5VU-fK",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ym0j6yRdw8J"
      },
      "source": [
        "# X5 Retail data"
      ],
      "id": "9Ym0j6yRdw8J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dated-anthropology"
      },
      "source": [
        "%matplotlib inline\n",
        "df_clients = pd.read_csv(data_dir + 'clients.csv', index_col='client_id')\n",
        "df_train = pd.read_csv(data_dir + 'uplift_train.csv', index_col='client_id')\n",
        "df_test = pd.read_csv(data_dir + 'uplift_test.csv', index_col='client_id')\n",
        "\n",
        "\n",
        "df_features = df_clients.copy()\n",
        "df_features['first_issue_time'] = \\\n",
        "    (pd.to_datetime(df_features['first_issue_date'])\n",
        "     - pd.to_datetime(df_features['first_issue_date']).min()) / pd.Timedelta('365d')\n",
        "\n",
        "df_features['first_redeem_time'] = \\\n",
        "    (pd.to_datetime(df_features['first_redeem_date'])\n",
        "     - pd.to_datetime(df_features['first_redeem_date']).min()) / pd.Timedelta('365d')\n",
        "\n",
        "df_features['issue_redeem_delay'] = df_features['first_redeem_time'] \\\n",
        "    - df_features['first_issue_time']\n",
        "\n",
        "df_features = df_features.join(pd.get_dummies(df_features['gender']))\n",
        "df_features['first_redeem_time'] = df_features['first_redeem_time'].fillna(df_features['first_redeem_time'].mean())\n",
        "df_features['issue_redeem_delay'] = df_features['issue_redeem_delay'].fillna(df_features['issue_redeem_delay'].mean())\n",
        "\n",
        "df_features = df_features.drop(['first_issue_date', 'first_redeem_date', 'gender'], axis=1)\n",
        "\n",
        "indices_train = df_train.index\n",
        "indices_test = df_test.index\n",
        "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
        "\n",
        "\n",
        "X_train = df_features.loc[indices_learn, :]\n",
        "y_train = df_train.loc[indices_learn, 'target']\n",
        "treat_train = df_train.loc[indices_learn, 'treatment_flg']\n",
        "\n",
        "X_val = df_features.loc[indices_valid, :]\n",
        "y_val = df_train.loc[indices_valid, 'target']\n",
        "treat_val =  df_train.loc[indices_valid, 'treatment_flg']\n",
        "\n",
        "X_train_full = df_features.loc[indices_train, :]\n",
        "y_train_full = df_train.loc[:, 'target']\n",
        "treat_train_full = df_train.loc[:, 'treatment_flg']\n",
        "\n",
        "X_test = df_features.loc[indices_test, :]\n",
        "\n",
        "cat_features = ['gender']"
      ],
      "id": "dated-anthropology",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SET0WhHNRv0y"
      },
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "id": "SET0WhHNRv0y",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRRkknrWYSRT"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "train_dataset = RealDataset(X_train, y_train, treat_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = RealDataset(X_val, y_val, treat_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "id": "uRRkknrWYSRT",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih15gQ99R4ka"
      },
      "source": [
        "input_dim = 8\n",
        "hidden_dims = [20, 20, 30, 10]\n",
        "dims = [input_dim] + hidden_dims + [1]\n",
        "\n",
        "layers = []\n",
        "for i in range(len(dims) - 1):\n",
        "    layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "    layers.append(nn.LeakyReLU())\n",
        "layers[-1] = nn.Sigmoid()\n",
        "model = nn.Sequential(*layers).to(device)"
      ],
      "id": "Ih15gQ99R4ka",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIs3-KJ5YxYx"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "id": "TIs3-KJ5YxYx",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Uc8HX2S7CG",
        "outputId": "2c542d5c-62d1-4ace-e037-96b4b893816a"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "alpha = 0.8\n",
        "p = 0.5\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_upl_at_k = []\n",
        "test_quini = []\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "\n",
        "    loss_train = []\n",
        "\n",
        "    for data0, data1, target, treat in train_loader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        preds0 = model(data0).squeeze()\n",
        "        preds1 = model(data1).squeeze()\n",
        "        loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_train.append(loss.item())\n",
        "    loss_train = np.mean(loss_train)\n",
        "\n",
        "    if i % 10 == 0 or i == N_EPOCHS - 1:\n",
        "        with torch.no_grad():\n",
        "            labels_val = []\n",
        "            preds_val = []\n",
        "            loss_val = []\n",
        "            upl_at_k_scores = []\n",
        "            quini_scores = []\n",
        "            for data0, data1, target, treat in val_loader:\n",
        "                labels_val += list(target)\n",
        "                preds0 = model(data0).squeeze()\n",
        "                preds1 = model(data1).squeeze()\n",
        "                loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p)\n",
        "                preds_val += list(preds1 - preds0)\n",
        "                loss_val.append(loss.item())\n",
        "\n",
        "                score = uplift_at_k(target.cpu().numpy(), preds1 - preds0, treat.cpu().numpy(), strategy='by_group', k=0.3)\n",
        "                upl_at_k_scores.append(score)\n",
        "                score = qini_auc_score(target.cpu().numpy(), preds1 - preds0, treat.cpu().numpy())\n",
        "                quini_scores.append(score)\n",
        "\n",
        "            train_losses.append(loss_train)\n",
        "            test_losses.append(np.mean(loss_val))\n",
        "            test_upl_at_k.append(np.mean(upl_at_k_scores))\n",
        "            test_quini.append(np.mean(quini_scores))\n",
        "\n",
        "        print(f'Epoch {i} \\t Loss train {loss_train:.6f} \\t Loss test {np.mean(loss_val):.6f} \\t Uplift at k test {np.mean(upl_at_k_scores):.6f} \\t Quini AUC test {np.mean(quini_scores):.6f}')"
      ],
      "id": "l4Uc8HX2S7CG",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Loss train 0.669161 \t Loss test 0.668778 \t Uplift at k test 0.034172 \t Quini AUC test 0.004376\n",
            "Epoch 10 \t Loss train 0.659441 \t Loss test 0.660955 \t Uplift at k test 0.045266 \t Quini AUC test 0.005313\n",
            "Epoch 20 \t Loss train 0.656509 \t Loss test 0.655775 \t Uplift at k test 0.039694 \t Quini AUC test 0.007705\n",
            "Epoch 30 \t Loss train 0.655402 \t Loss test 0.655229 \t Uplift at k test 0.039955 \t Quini AUC test 0.006445\n",
            "Epoch 40 \t Loss train 0.655189 \t Loss test 0.656509 \t Uplift at k test 0.043516 \t Quini AUC test 0.008246\n",
            "Epoch 50 \t Loss train 0.654596 \t Loss test 0.656391 \t Uplift at k test 0.045156 \t Quini AUC test 0.007553\n",
            "Epoch 60 \t Loss train 0.654396 \t Loss test 0.655177 \t Uplift at k test 0.047011 \t Quini AUC test 0.011538\n",
            "Epoch 70 \t Loss train 0.653965 \t Loss test 0.654941 \t Uplift at k test 0.044733 \t Quini AUC test 0.006785\n",
            "Epoch 80 \t Loss train 0.653526 \t Loss test 0.654740 \t Uplift at k test 0.054619 \t Quini AUC test 0.013001\n",
            "Epoch 90 \t Loss train 0.653283 \t Loss test 0.656252 \t Uplift at k test 0.053992 \t Quini AUC test 0.013397\n",
            "Epoch 99 \t Loss train 0.653260 \t Loss test 0.654787 \t Uplift at k test 0.053756 \t Quini AUC test 0.012763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUD2aL66QCWW"
      },
      "source": [
        "# Hillstrom data"
      ],
      "id": "TUD2aL66QCWW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwrgq-paQByH"
      },
      "source": [
        "df = pd.read_csv(data_dir + 'Hillstrom.csv')\n",
        "df.drop(['history_segment', \"conversion\", \"spend\"], axis=1, inplace=True)\n",
        "\n",
        "cat_cols = ['zip_code', 'channel']\n",
        "df_ohe = pd.get_dummies(df, columns=cat_cols)\n",
        "df_ohe.segment = df_ohe.segment.map({'Womens E-Mail': 1, 'Mens E-Mail': 1, 'No E-Mail': 0})\n",
        "\n",
        "X = df_ohe.drop('visit', axis=1)\n",
        "y = df_ohe['visit'].astype('int')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
        "\n",
        "treat_train = X_train['segment']\n",
        "treat_test = X_test['segment']\n",
        "\n",
        "X_train.drop(['segment'], axis=1, inplace=True)\n",
        "X_test.drop(['segment'], axis=1, inplace=True)"
      ],
      "id": "Cwrgq-paQByH",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IKmEYFI0HKd",
        "outputId": "6627da47-5410-4537-cd73-010673230300"
      },
      "source": [
        "len(X_train)"
      ],
      "id": "7IKmEYFI0HKd",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quxZDXV9R1sU"
      },
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "id": "quxZDXV9R1sU",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azWI-OYzQsMQ"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "train_dataset = RealDataset(X_train, y_train, treat_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = RealDataset(X_test, y_test, treat_test)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "id": "azWI-OYzQsMQ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUfLFlCrQ0XP"
      },
      "source": [
        "input_dim = X_train.values.shape[1] + 1\n",
        "hidden_dims = [20, 10, 10]\n",
        "dims = [input_dim] + hidden_dims + [1]\n",
        "\n",
        "layers = []\n",
        "for i in range(len(dims) - 1):\n",
        "    layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "    layers.append(nn.LeakyReLU())\n",
        "layers[-1] = nn.Sigmoid()\n",
        "model = nn.Sequential(*layers).to(device)"
      ],
      "id": "HUfLFlCrQ0XP",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLi7scbkSwhX"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "id": "hLi7scbkSwhX",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKLNRMRIQ76y",
        "outputId": "80820f86-018a-4b82-eb61-47109a656ab0"
      },
      "source": [
        "N_EPOCHS = 150\n",
        "alpha = 0.8\n",
        "p = 0.5\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_upl_at_k = []\n",
        "test_quini = []\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "\n",
        "    loss_train = []\n",
        "\n",
        "    for data0, data1, target, treat in train_loader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        preds0 = model(data0).squeeze()\n",
        "        preds1 = model(data1).squeeze()\n",
        "        loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p)# + 0.01 * sum([(i ** 2).sum() for i in model.parameters()])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_train.append(loss.item())\n",
        "    loss_train = np.mean(loss_train)\n",
        "\n",
        "    if i % 10 == 0 or i == N_EPOCHS - 1:\n",
        "        with torch.no_grad():\n",
        "            labels_val = []\n",
        "            preds_val = []\n",
        "            treats_val = []\n",
        "            loss_val = []\n",
        "\n",
        "            for data0, data1, target, treat in val_loader:\n",
        "                preds0 = model(data0).squeeze()\n",
        "                preds1 = model(data1).squeeze()\n",
        "                loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p)# + 0.01 * sum([(i ** 2).sum() for i in model.parameters()])\n",
        "\n",
        "                labels_val += list(target)\n",
        "                preds_val += list(preds1 - preds0)\n",
        "                treats_val += list(treat)\n",
        "                loss_val.append(loss.item())\n",
        "\n",
        "            upl_score = uplift_at_k(labels_val, preds_val, treats_val, strategy='by_group', k=0.3)\n",
        "            test_upl_at_k.append(upl_score)\n",
        "            quini_score = qini_auc_score(labels_val, preds_val, treats_val)\n",
        "            test_quini.append(quini_score)\n",
        "\n",
        "            train_losses.append(loss_train)\n",
        "            test_losses.append(np.mean(loss_val))\n",
        "\n",
        "        print(f'Epoch {i} \\t Loss train {loss_train:.6f} \\t Loss test {np.mean(loss_val):.6f} \\t Uplift at k test {upl_score:.6f} \\t Quini AUC test {quini_score:.6f}')"
      ],
      "id": "FKLNRMRIQ76y",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Loss train 0.602541 \t Loss test 0.550173 \t Uplift at k test 0.069443 \t Quini AUC test 0.015601\n",
            "Epoch 10 \t Loss train 0.468615 \t Loss test 0.465313 \t Uplift at k test 0.059740 \t Quini AUC test 0.009128\n",
            "Epoch 20 \t Loss train 0.463567 \t Loss test 0.462120 \t Uplift at k test 0.063688 \t Quini AUC test 0.012649\n",
            "Epoch 30 \t Loss train 0.461768 \t Loss test 0.459886 \t Uplift at k test 0.059206 \t Quini AUC test 0.008208\n",
            "Epoch 40 \t Loss train 0.460795 \t Loss test 0.459056 \t Uplift at k test 0.059631 \t Quini AUC test 0.011865\n",
            "Epoch 50 \t Loss train 0.460221 \t Loss test 0.458829 \t Uplift at k test 0.065846 \t Quini AUC test 0.013425\n",
            "Epoch 60 \t Loss train 0.459795 \t Loss test 0.458437 \t Uplift at k test 0.064113 \t Quini AUC test 0.014196\n",
            "Epoch 70 \t Loss train 0.459581 \t Loss test 0.458245 \t Uplift at k test 0.067861 \t Quini AUC test 0.014460\n",
            "Epoch 80 \t Loss train 0.459640 \t Loss test 0.458573 \t Uplift at k test 0.073718 \t Quini AUC test 0.024161\n",
            "Epoch 90 \t Loss train 0.459267 \t Loss test 0.457886 \t Uplift at k test 0.066136 \t Quini AUC test 0.021270\n",
            "Epoch 100 \t Loss train 0.459256 \t Loss test 0.458012 \t Uplift at k test 0.066138 \t Quini AUC test 0.012882\n",
            "Epoch 110 \t Loss train 0.458975 \t Loss test 0.457761 \t Uplift at k test 0.071432 \t Quini AUC test 0.024750\n",
            "Epoch 120 \t Loss train 0.458841 \t Loss test 0.458478 \t Uplift at k test 0.075072 \t Quini AUC test 0.029256\n",
            "Epoch 130 \t Loss train 0.458829 \t Loss test 0.458372 \t Uplift at k test 0.060422 \t Quini AUC test 0.013810\n",
            "Epoch 140 \t Loss train 0.458814 \t Loss test 0.458516 \t Uplift at k test 0.058622 \t Quini AUC test 0.016209\n",
            "Epoch 149 \t Loss train 0.458614 \t Loss test 0.460387 \t Uplift at k test 0.068790 \t Quini AUC test 0.012610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai7Eeoc2RLgf"
      },
      "source": [
        "# Kuusito data"
      ],
      "id": "Ai7Eeoc2RLgf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2oxPewyRLAQ"
      },
      "source": [
        "df = pd.read_csv(data_dir + 'Kuusito.csv')\n",
        "df.drop(['customer_type'], axis=1, inplace=True)\n",
        "\n",
        "df = df.replace(r'Value', '', regex=True)\n",
        "df['target_control'] = df['target_control'].map({'control': 1, 'target': 0})\n",
        "df['outcome'] = df['outcome'].map({'negative': 0, 'positive': 1})\n",
        "\n",
        "df = pd.get_dummies(df,drop_first=True)\n",
        "\n",
        "X = df.drop('outcome', axis=1).astype('int64')\n",
        "y = df['outcome']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
        "\n",
        "treat_train = X_train['target_control']\n",
        "treat_test = X_test['target_control']\n",
        "\n",
        "X_train.drop(['target_control'], axis=1, inplace=True)\n",
        "X_test.drop(['target_control'], axis=1, inplace=True)\n",
        "X_train.drop(['customer_id'], axis=1, inplace=True)\n",
        "X_test.drop(['customer_id'], axis=1, inplace=True)"
      ],
      "id": "o2oxPewyRLAQ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhh8w-OOz-HU",
        "outputId": "f10460de-440e-4f67-8045-4afb1682b5eb"
      },
      "source": [
        "len(X_train)"
      ],
      "id": "bhh8w-OOz-HU",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLjzImehR0w6"
      },
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "id": "VLjzImehR0w6",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lboJshLaRWED"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "train_dataset = RealDataset(X_train, y_train, treat_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = RealDataset(X_test, y_test, treat_test)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "id": "lboJshLaRWED",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhm0fCfIRc6i"
      },
      "source": [
        "input_dim = X_train.values.shape[1] + 1\n",
        "hidden_dims = [20, 10]\n",
        "dims = [input_dim] + hidden_dims + [1]\n",
        "\n",
        "layers = []\n",
        "for i in range(len(dims) - 1):\n",
        "    layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "    layers.append(nn.LeakyReLU())\n",
        "layers[-1] = nn.Sigmoid()\n",
        "model = nn.Sequential(*layers).to(device)"
      ],
      "id": "dhm0fCfIRc6i",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX0JWIv0yj-7"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "id": "BX0JWIv0yj-7",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwMyF49lRfGy",
        "outputId": "4d11e096-b063-4ab1-e05e-4549896da58e"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "alpha = 0.8\n",
        "p = 0.5\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_upl_at_k = []\n",
        "test_quini = []\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "\n",
        "    loss_train = []\n",
        "\n",
        "    for data0, data1, target, treat in train_loader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        preds0 = model(data0).squeeze()\n",
        "        preds1 = model(data1).squeeze()\n",
        "        loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p) + 0.001 * sum([(i ** 2).sum() for i in model.parameters()])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_train.append(loss.item())\n",
        "    loss_train = np.mean(loss_train)\n",
        "\n",
        "    if i % 10 == 0 or i == N_EPOCHS - 1:\n",
        "        with torch.no_grad():\n",
        "            labels_val = []\n",
        "            preds_val = []\n",
        "            loss_val = []\n",
        "            upl_at_k_scores = []\n",
        "            quini_scores = []\n",
        "            for data0, data1, target, treat in val_loader:\n",
        "                labels_val += list(target)\n",
        "                preds0 = model(data0).squeeze()\n",
        "                preds1 = model(data1).squeeze()\n",
        "                loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p) + 0.001 * sum([(i ** 2).sum() for i in model.parameters()])\n",
        "                preds_val += list(preds1 - preds0)\n",
        "                loss_val.append(loss.item())\n",
        "\n",
        "                score = uplift_at_k(target.cpu().numpy(), preds1 - preds0, treat.cpu().numpy(), strategy='by_group', k=0.3)\n",
        "                upl_at_k_scores.append(score)\n",
        "                score = qini_auc_score(target.cpu().numpy(), preds1 - preds0, treat.cpu().numpy())\n",
        "                quini_scores.append(score)\n",
        "\n",
        "            train_losses.append(loss_train)\n",
        "            test_losses.append(np.mean(loss_val))\n",
        "            test_upl_at_k.append(np.mean(upl_at_k_scores))\n",
        "            test_quini.append(np.mean(quini_scores))\n",
        "\n",
        "        print(f'Epoch {i} \\t Loss train {loss_train:.6f} \\t Loss test {np.mean(loss_val):.6f} \\t Uplift at k test {np.mean(upl_at_k_scores):.6f} \\t Quini AUC test {np.mean(quini_scores):.6f}')"
      ],
      "id": "EwMyF49lRfGy",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Loss train 0.700026 \t Loss test 0.697960 \t Uplift at k test -0.019132 \t Quini AUC test -0.015107\n",
            "Epoch 10 \t Loss train 0.678457 \t Loss test 0.691131 \t Uplift at k test 0.098858 \t Quini AUC test 0.054540\n",
            "Epoch 20 \t Loss train 0.656116 \t Loss test 0.698655 \t Uplift at k test 0.123377 \t Quini AUC test 0.071694\n",
            "Epoch 30 \t Loss train 0.638988 \t Loss test 0.708529 \t Uplift at k test 0.170455 \t Quini AUC test 0.095124\n",
            "Epoch 40 \t Loss train 0.626728 \t Loss test 0.717712 \t Uplift at k test 0.202880 \t Quini AUC test 0.106490\n",
            "Epoch 50 \t Loss train 0.616477 \t Loss test 0.739045 \t Uplift at k test 0.192995 \t Quini AUC test 0.105062\n",
            "Epoch 60 \t Loss train 0.610881 \t Loss test 0.738534 \t Uplift at k test 0.176353 \t Quini AUC test 0.100603\n",
            "Epoch 70 \t Loss train 0.606990 \t Loss test 0.751618 \t Uplift at k test 0.191715 \t Quini AUC test 0.099734\n",
            "Epoch 80 \t Loss train 0.600085 \t Loss test 0.755374 \t Uplift at k test 0.197397 \t Quini AUC test 0.105962\n",
            "Epoch 90 \t Loss train 0.598750 \t Loss test 0.761357 \t Uplift at k test 0.188167 \t Quini AUC test 0.103648\n",
            "Epoch 99 \t Loss train 0.593782 \t Loss test 0.772493 \t Uplift at k test 0.186279 \t Quini AUC test 0.097842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhq4n7nVdtfz"
      },
      "source": [
        "# Synthetic data"
      ],
      "id": "zhq4n7nVdtfz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtOIyPos8xz3"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SynthDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, target=None, treatment=None):\n",
        "        super(SynthDataset, self).__init__()\n",
        "        self.data = torch.from_numpy(data).type(torch.FloatTensor)\n",
        "        if target is not None:\n",
        "            self.target = torch.from_numpy(target).type(torch.FloatTensor)\n",
        "        if treatment is not None:\n",
        "            self.treatment = torch.from_numpy(treatment).type(torch.FloatTensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Make loader to give row with 0 treatment and row with 1 treatment for one query\n",
        "\n",
        "        m0 = torch.zeros((1, self.data.shape[1] + 1))\n",
        "        m0[:, :-1] = self.data[idx, :]\n",
        "        m0[:, -1] = 0\n",
        "        m1 = torch.zeros((1, self.data.shape[1] + 1))\n",
        "        m1[:, :-1] = self.data[idx, :]\n",
        "        m1[:, -1] = 1\n",
        "\n",
        "        if self.target is None:\n",
        "            return m0.squeeze().to(device), m1.squeeze().to(device)\n",
        "        else:\n",
        "            return (m0.squeeze().to(device), m1.squeeze().to(device), \n",
        "                    self.target[idx].to(device), self.treatment[idx].to(device))"
      ],
      "id": "PtOIyPos8xz3",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwogypxRRzky"
      },
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "id": "QwogypxRRzky",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VtE-UTV7twm"
      },
      "source": [
        "from causalml.dataset import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y, X, treatment, tau, b, e = synthetic_data(mode=2, n=50000, p=8, sigma=1.0)\n",
        "y = (y > np.median(y)).astype(int)\n",
        "X_train, X_test, y_train, y_test, treat_train, treat_test = train_test_split(X, y, treatment, test_size=0.33, random_state=0)\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_dataset = SynthDataset(X_train, y_train, treat_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = SynthDataset(X_test, y_test, treat_test)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "id": "3VtE-UTV7twm",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STcPh3_rYozr"
      },
      "source": [
        "input_dim = X.shape[1] + 1\n",
        "hidden_dims = [20, 20, 20, 10]\n",
        "dims = [input_dim] + hidden_dims + [1]\n",
        "\n",
        "layers = []\n",
        "for i in range(len(dims) - 1):\n",
        "    layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "    layers.append(nn.LeakyReLU())\n",
        "layers[-1] = nn.Sigmoid()\n",
        "model = nn.Sequential(*layers).to(device)"
      ],
      "id": "STcPh3_rYozr",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTGSZbUmdk6J"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "id": "iTGSZbUmdk6J",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFixVzlWdnjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405cf06d-7f9e-4985-97b8-299c2574228d"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "alpha = 0.8\n",
        "p = 0.5\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_upl_at_k = []\n",
        "test_quini = []\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "\n",
        "    loss_train = []\n",
        "\n",
        "    for data0, data1, target, treat in train_loader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        preds0 = model(data0).squeeze()\n",
        "        preds1 = model(data1).squeeze()\n",
        "        loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p) + 0.001 * sum([(i ** 2).sum() for i in model.parameters()])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_train.append(loss.item())\n",
        "    loss_train = np.mean(loss_train)\n",
        "\n",
        "    if i % 10 == 0 or i == N_EPOCHS - 1:\n",
        "        with torch.no_grad():\n",
        "            labels_val = []\n",
        "            loss_val = []\n",
        "            upl_at_k_scores = []\n",
        "            quini_scores = []\n",
        "            for data0, data1, target, treat in val_loader:\n",
        "                labels_val += list(target)\n",
        "                preds0 = model(data0).squeeze()\n",
        "                preds1 = model(data1).squeeze()\n",
        "                loss = indirect_loss_function(target, preds0, preds1, alpha, treat, p) + 0.001 * sum([(i ** 2).sum() for i in model.parameters()])\n",
        "                loss_val.append(loss.item())\n",
        "\n",
        "                score = uplift_at_k(target.cpu().numpy(), preds1 - preds0, treat.cpu().numpy(), strategy='by_group', k=0.3)\n",
        "                upl_at_k_scores.append(score)\n",
        "                score = qini_auc_score(target.cpu().numpy(), preds1 - preds0, treat.cpu().numpy())\n",
        "                quini_scores.append(score)\n",
        "\n",
        "            train_losses.append(loss_train)\n",
        "            test_losses.append(np.mean(loss_val))\n",
        "            test_upl_at_k.append(np.mean(upl_at_k_scores))\n",
        "            test_quini.append(np.mean(quini_scores))\n",
        "\n",
        "        print(f'Epoch {i} \\t Loss train {loss_train:.6f} \\t Loss test {np.mean(loss_val):.6f} \\t Uplift at k test {np.mean(upl_at_k_scores):.6f} \\t Quini AUC test {np.mean(quini_scores):.6f}')"
      ],
      "id": "mFixVzlWdnjv",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Loss train 0.577876 \t Loss test 0.518127 \t Uplift at k test 0.441825 \t Quini AUC test 0.177023\n",
            "Epoch 10 \t Loss train 0.487400 \t Loss test 0.491937 \t Uplift at k test 0.473219 \t Quini AUC test 0.203663\n",
            "Epoch 20 \t Loss train 0.485834 \t Loss test 0.491692 \t Uplift at k test 0.478475 \t Quini AUC test 0.206040\n",
            "Epoch 30 \t Loss train 0.485281 \t Loss test 0.490072 \t Uplift at k test 0.473175 \t Quini AUC test 0.206298\n",
            "Epoch 40 \t Loss train 0.484718 \t Loss test 0.489838 \t Uplift at k test 0.477778 \t Quini AUC test 0.206438\n",
            "Epoch 50 \t Loss train 0.485364 \t Loss test 0.491065 \t Uplift at k test 0.482662 \t Quini AUC test 0.204252\n",
            "Epoch 60 \t Loss train 0.485353 \t Loss test 0.489345 \t Uplift at k test 0.474764 \t Quini AUC test 0.205389\n",
            "Epoch 70 \t Loss train 0.484639 \t Loss test 0.489102 \t Uplift at k test 0.471595 \t Quini AUC test 0.205378\n",
            "Epoch 80 \t Loss train 0.484668 \t Loss test 0.489714 \t Uplift at k test 0.477067 \t Quini AUC test 0.204958\n",
            "Epoch 90 \t Loss train 0.484838 \t Loss test 0.489506 \t Uplift at k test 0.473858 \t Quini AUC test 0.204399\n",
            "Epoch 99 \t Loss train 0.484612 \t Loss test 0.491354 \t Uplift at k test 0.475083 \t Quini AUC test 0.205015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GXYtyay-YVb"
      },
      "source": [],
      "id": "_GXYtyay-YVb",
      "execution_count": null,
      "outputs": []
    }
  ]
}